{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8620a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d819795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_raw_csv_dataset_to_json():\n",
    "    with open(\"reviews.csv\") as csvfile:\n",
    "        with open(\"reviews.json\", \"w\") as f:\n",
    "            reader = csv.reader(csvfile)\n",
    "            data_json = {}\n",
    "            reviews = []\n",
    "            isFirstLine = True\n",
    "            for line in reader:\n",
    "                if isFirstLine:\n",
    "                    isFirstLine = False\n",
    "                    continue\n",
    "                row_num, state, area_name, reviewer_name = line[0], line[1], line[2], line[3]\n",
    "                review_date, rating, text = line[4], line[5], line[6]\n",
    "                review = {\n",
    "                    \"row_number\":row_num,\n",
    "                    \"state\":state,\n",
    "                    \"area_name\":area_name,\n",
    "                    \"reviewer_name\":reviewer_name,\n",
    "                    \"review_date\":review_date,\n",
    "                    \"rating\":rating,\n",
    "                    \"text\":text\n",
    "                }\n",
    "                reviews.append(review)\n",
    "            data_json[\"reviews\"] = reviews\n",
    "            json.dump(data_json, f)\n",
    "    return\n",
    "\n",
    "def load_untokenized_dataset(file_name=\"reviews.json\"):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        dataset = json.load(f)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e5ae30",
   "metadata": {},
   "source": [
    "# Data Exploration for Milestone 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713de516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStatesAreasSortedByNumReviews(dataset):\n",
    "    state_to_review_row_nums = defaultdict(list)\n",
    "    area_name_to_review_row_nums = defaultdict(list)\n",
    "    for i in range(len(dataset['reviews'])):\n",
    "        review = dataset['reviews'][i]\n",
    "        state_to_review_row_nums[review['state']].append(i)\n",
    "        area_name_to_review_row_nums[review['area_name']].append(i)\n",
    "    \n",
    "    state_names, num_reviews_by_state = [], []\n",
    "    for state_name, row_nums in state_to_review_row_nums.items():\n",
    "        state_names.append(state_name)\n",
    "        num_reviews_by_state.append(len(row_nums))\n",
    "    area_names, num_reviews_by_area = [], []\n",
    "    for area_name, row_nums in area_name_to_review_row_nums.items():\n",
    "        area_names.append(area_name)\n",
    "        num_reviews_by_area.append(len(row_nums))\n",
    "        \n",
    "    unsorted_states = [(state_names[i], num_reviews_by_state[i]) for i in range(len(state_names))]\n",
    "    unsorted_areas = [(area_names[i], num_reviews_by_area[i]) for i in range(len(area_names))]\n",
    "    sorted_states = sorted(unsorted_states, key=lambda x: x[1], reverse=True)\n",
    "    sorted_areas = sorted(unsorted_areas, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_states, sorted_areas\n",
    "\n",
    "def makePlot(sorted_lst, top, byState):\n",
    "    if top:\n",
    "        plt.bar([sorted_lst[i][0] for i in range(10)], [sorted_lst[i][1] for i in range(10)])\n",
    "    else:\n",
    "        plt.bar([sorted_lst[-i][0] for i in range(1,10)], [sorted_lst[-i][1] for i in range(1,10)])\n",
    "    plt.xticks(rotation=\"vertical\")\n",
    "    xlab = \"State\" if byState else \"Ski Area\"\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(\"Number of Reviews\")\n",
    "    q = \"largest\" if top else \"fewest\"\n",
    "    title = xlab + \"s with \" + q + \" number of reviews\"\n",
    "    plt.title(title)\n",
    "    return\n",
    "\n",
    "dataset_untokenized = load_untokenized_dataset()\n",
    "sorted_states, sorted_areas = getStatesAreasSortedByNumReviews(dataset=dataset_untokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e3cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makePlot(sorted_states, top=True, byState=True)\n",
    "# makePlot(sorted_states, top=False, byState=True)\n",
    "# makePlot(sorted_areas, top=True, byState=False)\n",
    "# makePlot(sorted_areas, top=False, byState=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507eabf",
   "metadata": {},
   "source": [
    "# End milestone 2\n",
    "# Begin tokenizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b84ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_write_dataset(dataset, tokenizer, file_name=\"dataset_tokenized.json\"):\n",
    "    dataset_tokenized = {}\n",
    "    for review in dataset[\"reviews\"]:\n",
    "        review_tokenized = {'state':review['state'], 'area_name':review['area_name'], 'reviewer_name':review['reviewer_name'],\n",
    "                            'review_date':review['review_date'], 'rating':review['rating']}\n",
    "        review_tokenized['tokens'] = tokenizer.tokenize(review['text'])\n",
    "        dataset_tokenized[review['row_number']] = review_tokenized\n",
    "    with open(file_name, \"w\") as f:\n",
    "        json.dump(dataset_tokenized, f)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8878c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenize_and_write_dataset(dataset=dataset, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a822843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenized_dataset(file_name=\"dataset_tokenized.json\"):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        dataset = json.load(f)\n",
    "    return dataset\n",
    "\n",
    "def printReview(dataset, n):\n",
    "    review = dataset[str(n)]\n",
    "    print(review['area_name'], review['state'])\n",
    "    print(review['rating'])\n",
    "    print(review['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e2d017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized = load_tokenized_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b74a81",
   "metadata": {},
   "source": [
    "In our tokenized dataset, we have the structure:\n",
    "{\n",
    "  \"0\": {\n",
    "    'state': \"california\",\n",
    "    'area_name': \"squaw-valley-usa\",\n",
    "    'reviewer_name': \"john smith\",\n",
    "    'review_date': \"31st December 2019\",\n",
    "    'rating': \"4\",\n",
    "    'tokens': [\"we\", \"went\", \"to\", ... ]\n",
    "  }\n",
    "  \"1\": {...}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8171db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(dataset):\n",
    "    \"\"\"\n",
    "    Return dict:\n",
    "    {\n",
    "        \"word\" : [(doc_num, tf1), ...]\n",
    "    }\n",
    "    \"\"\"\n",
    "    inv_idx = defaultdict(list)\n",
    "    for row_num, review in dataset.items():\n",
    "        tokens = review['tokens']\n",
    "        token_to_count = defaultdict(int)\n",
    "        for tok in tokens:\n",
    "            token_to_count[tok] += 1\n",
    "        for tok, tf in token_to_count.items():\n",
    "            inv_idx[tok].append((row_num, tf))\n",
    "    return inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a170b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_idx = build_inverted_index(dataset_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "473a8c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_idf(inv_idx, min_df, max_df_ratio, n_docs):\n",
    "    \"\"\"\n",
    "    Returns dict[term] = log(num_docs / (1+df))\n",
    "    \"\"\"\n",
    "    idf = {}\n",
    "    for term, lst in inv_idx.items():\n",
    "        df = len(lst)\n",
    "        if df >= min_df and (df/n_docs) <= max_df_ratio:\n",
    "            idf[term] = math.log(n_docs/(1+df), 2)\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c779bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = build_idf(inv_idx, min_df=10, max_df_ratio=0.3, n_docs=len(dataset_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21efbe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.867979416207268\n"
     ]
    }
   ],
   "source": [
    "# x = [(term, len(lst)) for term, lst in inv_idx.items()]\n",
    "# print(sorted(x, key=lambda l:l[1], reverse=True)[10:30])\n",
    "print(idf[\"mountain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ca90fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_doc_norms(inv_idx, idf, n_docs):\n",
    "    norms = np.zeros(n_docs)\n",
    "    for term, lst in inv_idx.items():\n",
    "        try:\n",
    "            idf_i = idf[term]\n",
    "            for row_num, tf in lst:\n",
    "                norms[int(row_num)] += (tf*idf_i)**2\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return [math.sqrt(i) for i in norms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7c91c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = build_doc_norms(inv_idx, idf, len(dataset_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5e554c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, inv_idx, idf, doc_norms, tokenizer=TreebankWordTokenizer()):\n",
    "    query = tokenizer.tokenize(query.lower())\n",
    "    tf_q = Counter(query)\n",
    "    numerators = [0 for i in range(len(doc_norms))] # intialize the score for each doc to 0\n",
    "    q_norm = 0\n",
    "    for term in tf_q:\n",
    "        if term not in idf:\n",
    "            continue\n",
    "        q_norm += ((tf_q[term]*idf[term]) ** 2)\n",
    "        for doc_num, tf in inv_idx[term]:\n",
    "            w_iq = tf_q[term] * idf[term]\n",
    "            w_ij = tf * idf[term]\n",
    "            numerators[int(doc_num)] += (w_iq*w_ij)\n",
    "    q_norm = math.sqrt(q_norm)\n",
    "    result = [(numerators[i]/(q_norm*doc_norms[i]), i) for i in range(len(doc_norms)) if doc_norms[i] != 0]\n",
    "    return sorted(result, key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c40495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"The biggest mountain on the east coast\"\n",
    "results = search(query, inv_idx, idf, norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fdcdda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********\n",
      "The biggest mountain on the east coast\n",
      "0.49430364841221247 giants-ridge-resort\n",
      "0.49430364841221247 jay-peak\n",
      "0.44832083893128866 mad-river-glen\n",
      "0.44832083893128866 mad-river-glen\n",
      "0.39426417828179083 jackson-hole\n",
      "0.3370198379857671 mt-baker\n",
      "0.33613188983556824 sunday-river\n",
      "0.32230170141813014 sunday-river\n",
      "0.30954305627785605 sunday-river\n",
      "0.2889183871285024 elk-mountain-ski-resort\n"
     ]
    }
   ],
   "source": [
    "print(\"***********\")\n",
    "print(query)\n",
    "for score, idx in results[:10]:\n",
    "    print(score, dataset_tokenized[str(idx)][\"area_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183b88d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4300-env",
   "language": "python",
   "name": "cs4300-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
